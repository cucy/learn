# 并发

了解并发
并发性本质上是同时做多个事情的实践，但不是并行的。它可以帮助我们提高应用程序的感知性能，还可以提高应用程序运行的速度。

考虑并发工作的最佳方式是想象一个人在多个任务上工作，并在这些任务之间快速切换。假设这个人同时在一个程序中工作，同时处理支持请求。这个人将主要关注他们的程序的编写，快速的上下文切换到修复bug或处理支持问题，应该有一个。一旦他们完成了支持任务，他们就可以再次切换上下文，回到快速编写程序。

然而，在计算方面，通常有两个性能瓶颈，我们在编写程序时必须注意和防范。了解两个瓶颈之间的差异是很重要的，就好像您试图将并发应用于基于cpu的瓶颈，然后您会发现程序实际上开始看到性能下降，而不是增加。如果您试图将并行性应用于真正需要并发解决方案的任务，那么您可以再次看到相同的性能命中。


# 并发系统的属性

所有并发系统都拥有相似的属性集;这些可定义如下:

多个参与者:这代表了不同的进程和线程，它们都试图在自己的任务上积极地取得进展。我们可以有多个进程，其中包含多个试图同时运行的线程。
共享资源:该特性表示前一组参与者必须使用的内存、磁盘和其他资源，以便执行他们需要做的事情。
规则:这些是所有并发系统必须遵循的严格规则集，并且定义了什么时候行动者可以并且不能获取锁、访问内存、修改状态等等。这些规则对于这些并发系统的工作至关重要，否则，我们的程序就会分崩离析。


# I / O瓶颈

I/O瓶颈，或者简称I/O瓶颈，是你的计算机花更多时间等待各种输入和输出而不是处理信息的瓶颈。

当您使用I/O重应用程序时，您通常会发现这种类型的瓶颈。我们可以将您的标准web浏览器作为重I/O应用程序的示例。在浏览器中，我们通常花费大量的时间等待网络请求完成，比如样式表、脚本或HTML页面，而不是在屏幕上呈现。

如果请求数据的速率低于被请求的速率，那么就会出现I/O瓶颈。

提高这些应用程序速度的主要方法之一是通过购买更昂贵和更快的硬件来提高底层I/O的速度，或者改进我们处理这些I/O请求的方式。

一个由I/O瓶颈约束的程序的一个很好的例子就是web爬虫。现在，web爬虫的主要目的是遍历web，并且本质上是索引web页面，以便在谷歌运行其搜索排名算法时可以考虑它们，以决定给定关键字的前10个结果。

我们将从创建一个非常简单的脚本开始，该脚本只请求一个页面，并乘以请求所需要的时间，如下所示:



# 理解并行性
在第一章中，我们讨论了Python的多处理功能，以及如何利用这一点来利用硬件中更多的处理核心。但是，当我们说我们的程序并行运行时，我们的意思是什么呢?

并行性是同时执行两个或多个动作的艺术，而不是同时在两个或多个事物上取得进展。这是一个重要的区别，为了实现真正的并行性，我们需要多个处理器同时运行我们的代码。

对并行处理的一个很好的类比是，考虑到可乐的队列。如果你有两排20人的队伍，他们都在等着使用可乐机，这样他们就能在剩下的时间里吃点糖，好吧，这就是一个并发性的例子。现在，假设你要在混合中引入第二个可乐机——这将会是一个平行发生的例子。这就是并行处理的工作原理——在那个房间里的每一个可乐机都代表一个处理核心，并且能够同时在任务上取得进展:
https://github.com/montanaflynn/programming-articles/blob/master/articles/parallelism-and-concurrency-need-different-tools.md


一个真实的例子突出了并行处理的真正力量是你的计算机的图形卡。这些图形卡往往有数百个(如果不是数千个)独立的独立处理核心，并且可以同时计算东西。我们之所以能够以如此平滑的帧率运行高端电脑游戏，是因为我们已经能够将如此多的并行内核放到这些卡片上。



# CPU瓶颈
通常，cpu限制的瓶颈是I/ o绑定瓶颈的逆。这一瓶颈出现在大量处理大量数据的应用程序中，或者任何其他计算成本较高的任务。这些程序的执行速度取决于CPU的速度——如果你在你的机器上扔一个更快的CPU，你应该会看到这些程序的速度直接增加。

如果处理数据的速度远远超过请求数据的速率，那么就会出现cpu受限的瓶颈。
在第一章，加快速度!当我们试图计算10,000个大随机数的主要因素时，我们快速地研究了如何与CPU绑定的程序进行战斗，这是一个严重依赖CPU的问题。然后，我们以一种使我们能够利用更多CPU的方式实现了相同的质数分解程序，从而直接提高了程序执行的速度。

# 它们如何在CPU上工作?
理解上一节中介绍的并发性和并行性之间的区别是非常重要的，但是了解更多关于您的软件将要运行的系统也是非常重要的。了解不同的体系结构风格以及底层机制有助于您在软件设计中做出最明智的决定。


# 单核cpu

单核处理器只会在给定的时间执行一个线程，因为这是它们所能做到的。然而，为了确保我们不会看到我们的应用程序被挂起，并且没有响应，这些处理器会在多个执行线程之间快速切换数千次每秒。线程之间的这种切换称为“上下文切换”，它涉及在特定的时间点将所有必需的信息存储到一个特定的点，然后在另一个不同的点上恢复它。

使用这种不断保存和恢复线程的机制，我们可以在一个给定的秒内在相当多的线程上取得进展，而且看起来就像计算机同时在做很多事情一样。事实上，它在任何给定的时间只做一件事，但是以这样的速度来做它对那台机器的用户来说是难以察觉的。

当在Python中编写多线程应用程序时，需要注意的是，这些上下文切换在计算上非常昂贵。不幸的是，现在还没有办法解决这个问题，现在的操作系统的大部分设计都是为了优化这些上下文切换，这样我们就不会有那么多的痛苦了。

以下是单核cpu的优点:
    它们不需要在多核之间进行任何复杂的通信协议。
    单核cpu需要更少的电源，这使得它们更适合于物联网设备。
然而，单核cpu有以下缺点:
    它们在速度上是有限的，而更大的应用程序会使它们陷入困境并可能被冻结。
    散热问题对单核CPU的运行速度有很大的限制。


   
# 时钟频率
在机器上运行的单核心应用程序的一个关键限制是CPU的时钟速度。当我们讨论时钟频率时，我们实际上是在讨论一个CPU每秒可以执行多少个时钟周期。

在过去的10年里，我们目睹了制造商们成功地超越了摩尔定律(Moore’s law)，这基本上是一种观察，即一个人能够在一块硅片上放置的晶体管数量大约每两年增加一倍。

每两年增加一倍的晶体管为单cpu时钟频率的指数增益铺平了道路，而cpu从低的MHz向4-5 GHz的时钟速度前进，我们现在在英特尔的i7 6700k处理器上看到了这一速度。

但是，随着晶体管的体积缩小到几纳米，这将不可避免地结束。我们已经开始触及物理学的边界，不幸的是，如果我们再缩小一点，我们就会开始受到量子隧穿效应的影响。由于这些物理限制，我们需要开始研究其他方法以提高我们能够计算事物的速度。

这就是马特里的可扩展性模型发挥作用的地方。  (Materlli's Model of Scalability )


# 马特利模型的可伸缩性
《Python Cookbook》一书的作者Alex Martelli提出了一个可扩展性的模型，Raymond Hettinger在他长达一个小时的关于“思考并发”的演讲中讨论了他在PyCon俄罗斯2016年所做的事情。这个模型代表了三种不同类型的问题和程序:

1核心:这是指单线程和单进程程序。
2-8核:这是指多线程和多处理程序。
9+核心:这指的是分布式计算。
第一类是单线程的单线程类别，它能够处理越来越多的问题，因为单内核cpu的速度不断提高，因此，第二个类别变得越来越过时。我们最终将会达到一个极限，以一个2-8的核心系统可以运行的速度，然后我们将不得不开始寻找其他的方法，例如多个CPU系统，甚至是分布式计算。

如果您的问题值得快速解决，并且需要大量的力量，那么明智的方法是使用分布式计算类别，并将多个机器和多个实例进行自旋，以真正并行地解决您的问题。处理数以亿计请求的大型企业系统是这个类别的主要居民。您通常会发现，这些企业系统部署在数十个，如果不是数百个，高性能、功能强大的服务器，遍布世界各地。



# 多核处理器

我们现在已经了解了单核处理器是如何工作的，但现在是时候看看多核处理器了。多核处理器包含多个独立的处理单元或“核心”。每个核心都包含它所需要的一切，以便执行一个存储指令序列。这些核心都遵循自己的周期，包括以下过程:

Fetch:这个步骤涉及从程序内存获取指令。这是由程序计数器(PC)决定的，它标识执行下一步的位置。
解码:核心转换它刚刚获取的指令，并将其转换成一系列信号，这些信号将触发CPU的其他部分。
执行:最后，执行执行步骤。这是我们运行我们刚刚获取和解码的指令的地方，然后将执行的结果存储在一个CPU寄存器中。
拥有多个内核为我们提供了在多个Fetch ->解码->执行周期独立工作的优势。这种风格的体系结构使我们能够创建更高性能的程序来利用并行执行。

以下是多核处理器的优点:

我们不再受限于单内核处理器所绑定的性能限制。
如果设计良好，能够利用多个核心的应用程序将会运行得更快。
然而，这些都是多核处理器的缺点:

它们比典型的单核处理器需要更多的能量。
跨核心沟通绝非易事;我们有多种不同的方法，在这一章后面我会详细讲解。


# 系统架构风格
在设计程序时，需要注意的是，有许多不同的内存架构风格适合不同的用例的需求。一种类型的内存架构对于并行计算任务和科学计算很有帮助，但在标准的家庭计算任务中却有点麻烦。

当我们对这些不同的风格进行分类时，我们倾向于遵循一种分类法，该分类法最初是由一个叫迈克尔·弗林的人在1972年提出的。这种分类法定义了四种不同风格的计算机体系结构。这些都是:

单指令流，单数据流。
SIMD:单指令流，多数据流。
MISD:多条指令流，单数据流。
多指令流，多数据流。
SISD: single instruction stream, single data stream
SIMD: single instruction stream, multiple data stream
MISD: multiple instruction stream, single data stream
MIMD: multiple instruction stream, multiple data stream

我们将在下面的部分中详细讨论这些体系结构。

# SISD
单指令流，单数据流往往是您的单处理器系统。这些系统有一个连续的数据流进入它们，以及一个用于执行此流的单个处理单元。

这种风格的架构典型地代表了您的经典的冯•诺伊曼机器，在多核处理器流行之前，这代表了您典型的家用计算机。您将拥有一个处理您所需要的一切的单一处理器。但是，这些都无法实现诸如指令并行性和数据并行性之类的东西，而诸如图形处理之类的东西对这些系统来说是极其繁重的。

下图显示了单处理器系统的外观。它具有一个由单个处理单元处理的数据源:


# SIMD

SIMD(单指令流、多数据流)archtecture，多数据流架构最适合于处理大量多媒体的系统。这些都是做3D图形的理想方法，因为它们可以操纵向量。例如，假设您有两个不同的数组，[10、15、20、25]和[20、15、10、5]。在SIMD体系结构中，您可以将这些添加到一个操作中，以获得[30、30、30、30]。如果我们要在标量架构上这样做，我们必须执行四个不同的添加操作，如下图所示:

这种风格的体系结构最好的例子可以在图形处理单元中找到。在OpenGL图形编程中，你有一个被称为顶点数组对象或VAOs的对象，这些VAOs通常包含多个顶点缓冲对象，用于描述游戏中的任何给定的3D对象。如果有人要移动他们的字符，每个顶点缓冲对象中的每个元素都必须快速地重新计算，以便让我们看到字符在屏幕上平滑地移动。

这就是SIMD架构的威力所在。我们把所有的元素都传递给不同的VAOs。一旦这些VAOs被填充，我们就可以告诉它我们想要在这个VAO中乘以一个旋转矩阵。然后，很快就可以在每个元素上执行相同的操作，这比非向量架构的效率要高得多。
下一个图展示了SIMD体系结构的高级概述。我们有多个数据流，可以表示多个向量，以及多个处理单元，它们都能够在任何给定的时间内执行单个指令。图形卡通常有数百个独立的处理单元:

SIMD的主要优点如下:

我们可以使用一个指令对多个元素执行相同的操作。
随着现代显卡核心数量的增加，这些卡片的吞吐量也会增加，这要归功于这种架构。
我们将在第11章使用GPU来充分利用这种风格的架构。

# MISD

多个指令流、单个数据流或MISD是一种有点不受欢迎的体系结构，目前还没有实际可用的实例。通常很难找到一个用例，在这种情况下，MISD体系结构样式是合适的，并且会很好地解决问题。

今天没有任何一个关于MISD体系结构的真实例子。

# MIMD 多指令多数据
多个指令流，多个数据流是最多样化的分类，封装了所有现代的多核处理器。组成这些处理器的每个内核都能够独立运行并并行运行。与我们的SIMD机器相比，基于mimd的机器能够在多个数据集上并行运行多个不同的操作，而不是在多个数据集上执行单个操作。

下一个图展示了许多不同处理单元的示例，它们都有许多不同的输入数据流，它们都独立地执行:



# 计算机内存体系结构风格
当我们开始通过引入诸如并发性和并行性这样的概念来加速我们的程序时，我们就会开始面临新的挑战，这些挑战必须被适当地考虑和处理。我们开始面临的最大挑战之一是我们访问数据的速度。在这个阶段需要注意的是，如果我们不能足够快地访问数据，那么这将成为我们的程序的瓶颈，不管我们如何熟练地设计我们的系统，我们将永远看不到任何性能提升。

计算机设计者们越来越多地寻找方法来改进我们可以开发新的并行解决方案的方法。通过提供一个单一的物理地址空间，我们的多个内核都可以在一个处理器中访问，这是他们设法改进的方法之一。这就像程序员一样，从我们这里消除了一定数量的复杂性，并允许我们专注于确保我们的代码是线程安全的。

在各种不同的场景中，有许多不同风格的体系结构。系统设计者所采用的主要两种不同的架构风格往往是遵循统一的内存访问模式或非统一的内存访问模式，或者分别是UMA和NUMA。



UMA
UMA (Uniform Memory Access) is an architecture style that features a shared memory space that can be utilized in a uniform manner by any number of processing cores. In layman's terms this means that regardless of where that core resides, it will be able to directly access a memory location in the same time no matter how close the memory is. This style of architecture is also known as Symmetric Shared-Memory Multiprocessors or SMP in short.

The following image depicts how a UMA-style system would piece together. Each processor interfaces with a bus, which performs all of the memory accessing. Each processor added to this system increases the strain on the bus bandwidth, and thus we aren't able to scale it in quite the same way we could if we were to use a NUMA architecture:

UMA(统一内存访问)是一种架构风格，它具有一个共享的内存空间，可以通过任意数量的处理核心以统一的方式使用。在外行人的术语中，这意味着无论核心驻留在何处，它都能够在相同的时间内直接访问内存位置，无论内存有多接近。这种类型的体系结构也称为对称共享内存多处理器或简称SMP。
下面的图像描述了一个uma风格的系统是如何拼接在一起的。每个处理器与总线接口，它执行所有的内存访问。添加到这个系统的每个处理器增加了总线带宽上的压力，因此我们不能以与使用NUMA体系结构相同的方式来扩展它:

UMA的优点如下:

所有RAM访问的时间都是相同的。
缓存是前后一致的。
硬件设计更简单

然而，有一个缺点:

UMA系统有一个内存总线，所有系统都可以访问内存;不幸的是，这带来了伸缩性问题。


# NUMA
NUMA(非统一内存访问)是一种体系结构样式，其中某些内存访问可能比其他内存访问速度快，这取决于处理器的请求，这可能是由于处理器的位置与内存有关。Show next是一个图表，它显示了在NUMA风格中许多处理器是如何相互连接的。每个都有自己的缓存，访问主内存和独立的I/O，并且每个都连接到互连网络:



Source: https://virtualizationdeepdive.wordpress.com/deep-dive-numa-vnuma/There is one major advantage of NUMA:NUMA machines are more scalable than their uniform-memory access counterpartsThe following are the disadvantages of NUMA:Non-deterministic memory access times can lead to either really quick access times if memory is local, or far longer times if memory is in distant memory locationsProcessors must observe the changes made by other processors; the amount of time it takes to observe increases in relation to how many processors are part of it

NUMA的主要优点是:

NUMA机器比它们的统一内存访问副本更可伸缩。
以下是NUMA的缺点:

如果内存是本地的，那么不确定的内存访问时间可能会导致快速访问时间，如果内存位于遥远的内存位置，则时间会更长。
处理器必须观察其他处理器的变化;观察到有多少处理器是它的一部分的时候，观察的时间增加了。


# Threads in Python

在我们详细讨论线程的生命周期之前，我觉得很重要的一点是，要知道我们将以实际的方式进行实例化。但是，为了了解这一点，我们需要了解Python的线程类定义，它可以在线程中找到。在这个文件中，您应该看到Thread类的类定义。它有一个构造函数，它看起来像这样:


```python
# Python Thread class Constructor
def __init__( self,
        group=None,   # 这是为将来的扩展预留的一个特殊参数。
        target=None,  # This is the callable object to be invoked by the run() method. If not passed, this will default to None, and nothing will be started. 这是run()方法调用的可调用对象。如果没有通过，这将默认为None，并且什么都不会启动。
        name=None,    #  This is the thread name.
        args=(),      # This is the argument tuple for target invocation. It defaults to (). 这是用于目标调用的参数元组。它默认为()。
        kwargs=None,  # This is a dictionary of keyword arguments to invoke the base class constructor.
        verbose=None
        ):
```


# 线程状态

线程可以存在于五个不同的状态:
运行、不运行、运行、启动和结束。
当我们创建一个线程时，我们通常不会为这个线程分配任何资源。它不存在于任何状态，因为它没有初始化，它只能启动或停止。让我们快速浏览一下这五个状态:

新线程:在新线程状态下，线程尚未启动，而且还没有分配资源。它只是对象的一个实例。
Runnable:这是线程等待运行时的状态，它拥有所需的所有资源以进行处理，而唯一阻止它的是任务调度程序没有调度它运行。
运行:在这个状态中，线程取得了进展——它执行它所需要的任何任务，并由任务调度程序选择运行。
从这个状态，我们的线程可以进入一个死状态，如果我们选择杀死，它或者它可以进入一个不运行状态。
不运行:这是线程在某种程度上暂停的时候。这可能是由许多原因造成的，比如等待长时间运行I/O请求的响应。或者，在另一个线程完成其执行之前，它可能被故意阻塞。
死亡:线程可以通过两种方式之一到达这个状态。它可以像我们一样，死于自然原因，也可以自然死亡。后者对谋杀者构成了重大风险，但我们将在本章的结尾部分详细讨论这些风险。

e: http://www.iitk.ac.in/esc101/05Aug/tutorial/essential/threads/lifecycle.html



# 分叉  Forking
fork进程是创建给定进程的第二个精确副本。换句话说，当我们使用某个东西时，我们实际上克隆了它，然后将它作为我们刚刚克隆的进程的子进程运行。

这个新创建的进程有它自己的地址空间，以及父进程中执行的父数据和代码的精确副本。当创建时，这个新克隆将接受它自己的唯一进程标识符(PID)，并且独立于它被克隆的父进程。

但是，为什么要克隆一个现有的流程呢?如果您曾经做过任何形式的网站托管，那么您可能会遇到Apache。Apache大量使用forking来创建多个服务器进程。每个独立进程都能够在自己的地址空间中处理自己的请求。在这种情况下，这是很理想的，因为它给我们提供了一些保护，因为如果进程崩溃或死亡，与它同时运行的其他进程将不受影响，并且能够继续满足任何新的请求。



# Daemonizing线程     Daemonizing a thread

首先，在我们查看守护线程之前，我认为知道这些线程是什么很重要。守护进程线程本质上是没有定义端点的线程。它们会一直运行直到你的程序退出。

“为什么这是有用的吗?”,你可能会问。例如，您有一个负载平衡器，它向应用程序的多个实例发送服务请求。您可能有某种形式的注册表服务，让您的负载平衡器知道如何发送这些请求，但是这个服务注册表如何知道您的实例的状态呢?通常情况下，在这个场景中，我们会发送一个叫做heartbeat的东西，或者在一个正常的时间间隔内发送一个有生命的数据包，对我们的服务注册中心说:“嘿，我还是200!”

这个例子是我们应用程序中的守护线程的主要用例。我们可以将发送心跳信号的任务迁移到我们的服务注册中心到一个守护线程，并在应用程序启动时启动它。这个守护线程将在我们的程序的后台运行，并定期发送此更新，而不需要任何干预。更好的是，当我们的实例关闭时，我们的守护线程将被杀死，而不必担心它。



# Starting loads of threads
The first example we'll look at is how we can start numerous threads all at once. We can create multiple thread objects by using a for loop and then starting them within the same for loop. In the following example, we define a function that takes in an integer and which sleeps for a random amount of time, printing both when it is starting and ending.

We then create a for loop which loops up to 10, and create 10 distinct thread objects that have their target set to our executeThread function. It then starts the thread object we've just created, and then we print out the current active threads.
开始大量的线程

第一个例子是我们如何同时启动多个线程。我们可以使用for循环创建多个线程对象，然后在同一个for循环中启动它们。在下面的示例中，我们定义了一个函数，该函数接受一个整数，并且该函数在一个随机的时间内休眠，当它开始和结束时，都要打印。

然后我们创建一个for循环，该循环将循环到10，并创建10个不同的线程对象，这些对象将目标设置为executeThread函数。然后它启动我们刚刚创建的线程对象，然后打印当前的活动线程。



# 使用线程减慢程序。
在处理线程时，很重要的一点是，要知道启动数百个线程并将它们全部抛出一个特定的问题可能不会提高应用程序的性能。很有可能，如果你旋转了成百上千个线程，实际上，你可以绝对杀死性能。

在第一章，加快速度!我们讨论了如何使用多个进程来加速一个非常简单的质数分解程序，这个程序是计算密集型的。在我的机器上，通过添加这些多进程，我见证了一个良好的50-100%的速度增长，但是我们尝试的是，我们试图使这个多线程而不是多处理。让我们看一下这个例子:


# 获取活动线程的总数。
有时，当您想要查询应用程序的状态时，您可能需要查询当前在Python程序中运行的活动线程的数量。值得庆幸的是，Python的本地线程模块很容易让我们通过一个简单的调用来获得它，就像下面代码片段中演示的那样:



# 获取当前线程
为了快速简便地确定我们所使用的线程，我们可以使用thread .current_thread()函数，如下例所示:

# 主线程
所有的Python程序都至少有一个线程——这个唯一线程是主线程。在Python中，我们可以从任何地方调用一个名为main_thread()函数，以检索主线程对象。让我们看看这个例子:


# 列举所有线程
可能有一段时间需要枚举所有活动线程，以便执行查询所有活动线程的状态。但是，有时候，您可能会忽略在应用程序的某个特定点上哪些线程正在发挥作用。

幸运的是，Python本身允许我们查询所有的活动线程，然后很容易地枚举它们，这样我们就可以获得我们需要的信息，或者适当地杀死它们，等等。让我们来看一个例子:


# 识别线程
在某些情况下，作为开发人员，它对我们非常有帮助，能够区分不同的线程。在某些情况下，您的应用程序可能由数百个不同的线程组成，并且识别它们可能有助于减轻您在调试和识别底层程序问题时的痛苦。

在大型系统中，如果线程执行不同的任务，将线程划分为组是一个好主意。例如，您有一个应用程序，它们都侦听传入的股票价格变化，并试图预测该价格将在何处发生。例如，您可以在这里有两个不同的线程组:一组侦听更改，另一组执行必要的计算。

对于执行侦听的线程和执行计算的线程有不同的命名约定，可以使您的跟踪日志文件的工作变得更加容易。



# 终止一个线程
结束线程被认为是糟糕的实践，而我积极建议反对。实际上，Python并没有提供一个本机线程函数来杀死其他线程，因此应该立即挂起标记。您希望终止的这些线程可能拥有需要打开和关闭的关键资源，或者它们也可以是多个子线程的父线程。通过杀死父线程而不杀死它们的子线程，我们实际上创建了孤儿线程。

停止线程的最佳实践。
如果您需要某种形式的线程关闭机制，那么您的任务就是实现一种机制，该机制允许优雅的关闭，而不是直接杀死线程。

然而，确实存在一个变通方法;虽然线程可能不具有终止的本机机制，但进程确实有这样的机制。正如您现在应该知道的，进程本质上是线程的增强版本，虽然它可能不是理想的，但在某些情况下，您必须确保您的程序能够优雅地关闭，而这将自己作为一个远比实现您自己的线程终止的解决方案更干净的解决方案。让我们看另一个例子:



# 孤儿进程
孤儿进程是没有父进程的线程。他们占用系统资源并没有提供任何好处，而杀死他们的唯一方法就是列举活的线程然后杀死它们。

# 操作系统如何处理线程?

现在我们已经了解了线程的生命周期，很重要的一点是知道这些线程是如何在您的机器中工作的。如果要在设计高性能软件时做出正确的决策，那么理解多线程模型和Python线程如何映射到系统线程是非常重要的。


# 创建的进程和线程

正如我们所看到的，一个过程是一个简单线程的重量级版本，我们可以在一个进程中创建多个线程。它们可以执行比标准线程更好的cpu绑定任务，因为它们各自都有各自单独的GIL实例。

但是，需要注意的是，虽然这些可能在cpu限制的问题上要好得多，但是它们也更加资源密集。资源密集意味着它们也更贵，它们在飞行中旋转和快速消失也更贵。在下一个示例中，我们将研究旋转多个线程的性能影响，并将其与多个进程的旋转进行比较。


# 多线程模型
在第一章，加快速度!第一节简要介绍了并发性，我们讨论了在一台机器上有两种不同类型的线程。这些是用户线程和内核线程，知道这些映射是如何相互映射的，以及它们可以被映射到一起的不同方式是很有用的。总共有三种不同的映射方式:

一个用户线程到一个内核线程。
许多用户级线程到一个内核线程。
许多用户线程到许多内核线程。
在Python中，我们通常使用一个用户线程到一个内核线程映射，因此，在多线程应用程序中创建的每个线程都将占用计算机上的大量资源。

然而，在Python生态系统中确实存在一些模块，它们使您能够在程序中实现多线程的功能，而只保留一个线程。最大最好的例子之一是asyncio模块，我们将深入到第9章事件驱动的编程中。


# One-to-one thread mapping
In this mapping, we see one user-level thread being mapped directly to one kernel-level thread. One-to-one mappings can be expensive due to the inherent costs of creating and managing kernel-level threads, but they provide advantages in the sense that user-level threads are not subject to the same level of blocking as threads that follow a many-to-one mapping are subject to:
Source: http://www2.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/4_Threads.html

在这个映射中，我们看到一个用户级线程被直接映射到一个内核级线程。由于创建和管理内核级线程的固有成本，一对一的映射可能很昂贵，但是它们提供了一种优势，即用户级的线程不受相同级别的阻塞，因为遵循多对一映射的线程会受到:

# Many-to-one
In many-to-one mappings, we see many user-level threads being mapped to one solitary kernel-level thread. This is advantageous as we can manage user-level threads efficiently; however, should if the user-level thread is blocked, the other threads that are mapped to kernel-level thread will also be blocked:
Source: http://www2.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/4_Threads.html

在多对一映射中，我们看到许多用户级线程被映射到一个单独的内核级线程。这是有利的，因为我们可以有效地管理用户级线程;但是，如果用户级线程被阻塞，则映射到内核级线程的其他线程也将被阻塞:


Many-to-many
In this threading model, we see many user-level threads being mapped to many kernel-level threads. This presents itself as the solution to the shortcomings of the previous two models.

Individual user-level threads can be mapped to a combination of either a single kernel-level thread or multiple kernel threads. It provides us, as programmers, the ability to choose which user-level threads we wish to map to kernel-level threads, and, overall, entitle us to a great deal of power when trying to ensure the very highest of performances when working in a multithreaded environment:
Source: http://www2.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/4_Threads.html

在这个线程模型中，我们看到许多用户级线程被映射到许多内核级线程。这是对前两种模型的缺点的解决方案。

可以将单个用户级线程映射到单个内核级线程或多个内核线程的组合。它提供给我们，作为程序员，我们能够选择我们希望映射到内核级线程的用户级线程，并且，总体上，在尝试确保在多线程环境中工作时的最高性能时，我们有权获得大量的权力:



# 线程之间的同步

线程之间的同步
因此，您知道线程是什么，以及如何正确地以Python开始和结束它们，并且希望您已经开始认识到实现并发程序所需要的一些复杂性。但是，我们如何确保在不影响程序流程的情况下安全地实现多线程呢?在本章中，我们将介绍一些基本问题，如果不加以防范，这些问题可能会困扰多线程应用程序。

在讨论一些关键的同步原语之前，我们必须先了解一下使用这些原语可能会发生的一些问题。这将直接导致我们在设计并发系统(即死锁)时面临的最大和最可怕的问题之一。阐述僵局的一个最好的方法就是看看吃饭哲学家的问题。

在饮食哲学家的问题中，我们遇到了五位著名的哲学家，他们围坐在圆桌旁，吃着一碗意大利面。在每一个碗之间，有五个叉子，哲学家可以用来吃他们的食物。然而，出于某种奇怪的原因，这些哲学家们决定，为了吃他们的食物，他们每人都需要5个叉子中的两个。

然而，这些哲学家中的每一个人，都可能是在饮食或思考状态，当他们选择在他们面前吃食物时，他们必须首先获得左右叉。然而，当一个哲学家拿起叉子时，他们必须等到吃完后才能交出叉子。

这种饮食方式是一个问题，当五个哲学家都设法同时拿起他们的左手。
在前面的图中，我们看到了这样的情况。五名哲学家中的每一位都拿起了左边的叉子，现在正坐在那里思考，直到有了正确的叉子。因为每个哲学家都不会放弃他们的叉子，直到他们吃完为止，饭桌已经陷入了僵局，再也不会走得更远了。

这个问题说明了我们在设计自己的并发系统时可能遇到的一个关键问题，它依赖于关键的同步原语(锁)，以便正确地运行。在这个例子中，我们的分叉是我们的系统资源，每个哲学家都代表一个相互竞争的过程。


Source: http://www.cs.fsu.edu/~baker/realtime/restricted/notes/philos.html


# 临界区



# 共享资源和数据竞争。
在应用程序中实现并发时，我们需要防范的主要问题之一是竞态条件。这些竞争条件会削弱我们的应用程序，并导致难以调试的bug，甚至更难修复。为了避免这些问题，我们需要了解这些竞态条件是如何发生的，以及如何使用我们将在本章中介绍的同步原语来防范它们。

如果要在Python中创建线程安全、高性能的程序，理解同步和对您可用的基本原语是至关重要的。幸运的是，我们在线程Python模块中有许多不同的同步原语，它们可以帮助我们处理许多不同的并发情况。

在本节中，我将向您简要介绍所有可用的同步原语，以及几个简单的示例，说明如何在程序中使用它们。到最后，您应该能够实现自己的并发Python程序，该程序可以以线程安全的方式访问资源。



# 锁
当试图从多个执行线程访问共享资源时，锁是一种必不可少的机制。最好的方法是想象你有一个浴室和多个室友——当你想梳洗或洗个澡时，你会想锁上门，这样就没人能同时使用卫生间了。

在Python中，锁是一种同步原语，它可以让我们基本上锁定我们的浴室门。它可以是“锁定”状态，也可以是“解锁”状态，我们只能在“解锁”状态下获取锁。


# RLocks
Reentrant-locks，或称为RLocks，是与我们的标准锁原语非常相似的同步原语，但是如果线程已经拥有它，它可以被线程多次获取。

例如，例如，thread-1获得了RLock，因此，每当线程-1获得锁时，RLock原语中的计数器就会递增1。如果线程2试图来获取RLock，那么它就必须等到RLock的计数器下降到0才能获得它。线程2将进入一个阻塞状态，直到满足这个0条件为止。

为什么这有用呢?例如，当您想要在访问其他类方法的类中使用线程安全访问方法时，它可能会很有用。


RLocks versus regular locks
If we were to try and perform the same preceding program using a traditional lock primitive, then you should notice that the program never actually reaches the point where it's executing our modifyA() function. Our program would, essentially, go into a form of deadlock, as we haven't implemented a release mechanism that allows our thread to go any further. This is shown in the following code example:


# 信号量
在第一章中，我们讨论了并发的历史，我们讨论了Dijkstra算法。Dijkstra是从铁路系统中提取信号的人把它们翻译成我们可以在自己复杂的并发系统中使用的东西。

Semaphores有一个内部计数器，每当获取或释放调用时，它就会递增和递减。在初始化时，这个计数器默认为1，除非另外设置。如果计数器将下降到负整数值，则无法获得信号量。

假设我们用一个信号量保护了一个代码块，并将信号量的值设置为2。如果一个线程获得了信号量，那么这个信号量的值就会递减到1。如果另一个线程试图获取信号量，信号量的值将递减到0。此时，如果还有另一个线程出现，这个信号量将拒绝它的获取请求，直到有一个原始的两个线程被称为release方法，而计数器增加到之前的0。


类定义
Python信号量对象的类定义如下所示:


# 线程竞争

前面的例子要注意的一点是，如果通过注释掉self来删除线程的模拟阻塞。运行函数的随机延迟，然后，当你运行程序时，无论哪个线程获得信号量，最可能会卖掉所有的票。这是因为赢得信号量的线程处于优先位置，以便在其他线程能够之前重新获得锁。

# 有界的信号量
有界信号量几乎与正常信号量相同。除了以下几点:

有界信号量检查，以确保其当前值不超过其初始值。如果是这样，就会产生ValueError。在大多数情况下，信号量被用来保护有限的资源。
如果信号量多次释放，这是一个错误的信号。如果没有给出一个值，则默认值为1。

通常，这些有界的信号量可以在web服务器或数据库实现中找到，以防止在许多人同时尝试连接或尝试一次执行特定操作的情况下，资源耗尽。

一般来说，使用有界信号量相对于正常信号量是更好的做法。如果我们要更改我们的信号量示例的前面代码，以使用thread . boundedsemaphore(4)并再次运行它，我们将看到几乎完全相同的行为，只是我们对一些非常简单的编程错误进行了保护，否则这些错误将不会被捕获。


# 事件
事件是非常有用的，但同时也是多个线程并发运行的一种非常简单的通信形式。对于事件，一个线程通常会发出一个信号，表明事件已经发生，而其他线程正在积极地侦听这个信号。

事件本质上是具有内部标志的对象，它要么是真，要么是假。在我们的线程中，我们可以连续地轮询这个事件对象，以检查它位于哪个状态，然后选择以任何我们想要的方式来执行该标志更改状态时的行为。

在前一章中，我们讨论了如何没有真正的机制来在Python中直接杀死线程，这仍然是正确的。但是，我们可以利用这些事件对象，并且只要我们的事件对象未设置，我们的线程就会运行。虽然在发送SIGKILL信号的时候这并不是很有用，但是在某些情况下，它可以在需要优雅地关闭的情况下有用，但是在那里您可以等待一个线程在它终止之前完成它正在做的事情。

一个事件有四个公共功能，我们可以修改和利用它:

isSet():检查事件是否已设置。
set():设置事件。
clear():这重置了我们的事件对象。
等待():此块直到内部标志被设置为true。


# 障碍
障碍是在Python语言的第三个主要迭代中引入的同步原语，它解决的问题只能通过复杂的条件和信号量的混合来解决。

这些障碍是控制点，可以用来确保进程只由一组线程完成，在所有参与线程到达相同的点之后。

这可能听起来有点复杂和不必要，但是在某些情况下它可能非常强大，而且它确实可以减少代码的复杂性。


# 线程之间的通信

通信是并发系统中最重要的部分之一。如果没有实现适当的通信机制，我们通过使用并发性和并行性实现的任何性能提升都可能毫无用处。当涉及到线程和进程之间的通信时，通信是您必须克服的最大的挑战之一，并且在您投入使用之前，必须对所有可用的选项有一个很好的理解。

在这一章中，我们将讨论如何实现自己的通信机制，并讨论在何时何地使用这些机制。

我们将在本章中讨论以下主题:

Python中的标准数据结构，以及如何以线程安全的方式与它们交互。
使用队列的线程安全通信，以及如何有效地使用这些队列对象。
双端队列，以及它们与传统队列的区别。
我们如何利用所有这些新概念并构建我们自己的多线程网站爬虫?




# FIFO queues
FIFO (first in first out) queues to give them their full name, are the standard queue implementation that Python has to offer. They follow the exact same queueing mechanism that you would if you were, say, at the supermarket. The first person to reach the till would be attended to first, the second person waits and is served second, and so on.

Source: http://javaworldwide.blogspot.co.uk/2015/07/implementing-blocking-queue-in-java.html
Through following this mechanism, we ensure that our customers are treated fairly, and that you'll be able to reasonably estimate roughly how long it will take for you to get served if you were, say, 7th in the queue.



# LIFO queues

 LIFO (last in first out) queues, act in the opposite fashion to that of normal FIFO queues. To extend our supermarket analogy further, in using a LIFO queueing mechanism, we, essentially, serve the last person to join the queue before the existing members of the queue are served. As you can imagine, if this were a real-life supermarket, there would probably be a number of complaints put in by people who were spending hours sitting in the same queue.In LIFO queues, there is the distinct possibility that a couple of the first people to join the queue could remain in that position indefinitely as more and more people join the queue before they can be served. While this may not make sense as a queueing mechanism in the real world, LIFO has its advantages when it comes to programming.LIFO queues come in particularly handy when it comes to implementing artificial-intelligence-based algorithms such as depth-first search, depth-limited search, and so on. It also comes in very handy when you want to reverse the order of something--simply populate your LIFO queue with every element, and then pop them off again once you are done. The results of this are more clearly defined in the following illustration:
 Source: http://www.transtutors.com/homework-help/accounting/inventory-valuation-lifo/


 # Priority Queue优先队列

 如果我们远离超市的类比，现在想想机场安全区，有些人比普通顾客更重要。这些人都是飞行员，机组人员等等。在这些特殊的情况下，我们通常会把它们移动到队列的前面，这样它们就可以继续得到我们即将起飞的飞机，让它们准备起飞。

换句话说，我们在排队机制中给予他们某种形式的优先权。有时，在我们所开发的系统中，我们还需要适应某种形式的优先级机制，以便在不确定的时间段内，难以置信的重要任务不会被数以百万计的相对不重要的操作所困。这就是我们的PriorityQueue对象发挥作用的地方。

在PriorityQueue中，我们可以把我们放入队列中的所有东西作为权重，来说明它的重要性。我们可以像填充普通队列对象一样填充priorityqueue，除了使用元组，并将priority_number传递为tuple中的第一个值:(priority_number, data)。


# 全/空队列
我们需要能够限制我们的程序中队列的大小;如果我们让它们永远膨胀，那么理论上，我们就可以开始面对memoryerror。一个Python程序可以使用的内存数量受我们系统中可用的内存数量的限制。

通过限制队列的大小，我们能够有效地保护自己不受这些内存限制的影响。在本例中，我们将创建一个队列，并传入maxsize参数，该参数将被设置为0。然后，我们将继续创建四个不同的线程，每个线程将尝试用任意数量填充这个队列。

然后，我们将加入所有新创建的线程，并尝试将尽可能多的元素放入队列中。


# Defining your own thread-safe communication structures 定义自己的线程安全通信结构。

一个网络爬虫的例子
现在，我们已经掌握了我们的通信原语和我们在前一章中处理的同步原语，现在是时候开始使用它们了。

还有什么比用它来制造有趣的东西更好的方法来实践我们新发现的知识呢?

在本章的这一节中，我们将构建一个非常简单的多线程web爬虫。













# Reactive Programming
While event-driven programming might revolve around events, in reactive programming, we deal purely with data. In other words, every time we receive a new piece of data, we consider this to be an event. Due to this definition, you could technically call it a branch of event-driven programming. However, due to its popularity and the differences in the way it does things, I couldn't help but put reactive programming in a chapter of its own.

In this chapter, we will dive deeper into one of the most popular libraries available in Python when it comes to reactive programming, RxPY. We'll cover in depth some of the features of this library and how we can utilize this to create our own asynchronous programs.

We'll come to terms with some of the basics necessary of RxPY to get us started:

Dealing with observers and observables
Lambda functions and how we can use them
The multitude of operators and how we can chain these to achieve a desired state
The differences between both hot and cold observables
Multicasting
We'll also take a brief look at the PyFunctional library, and how this differs from RxPY and how we can leverage that in certain scenarios. You should note that some of these examples from the official documentation have also been covered in a video course called Reactive Python for Data Science by Thomas Nield. I highly recommend this course as Thomas covers a lot of material that I've not had a chance to in this chapter. You can find this course at http://shop.oreilly.com/product/0636920064237.do.

反应性编程
虽然事件驱动的编程可能围绕事件进行，但在无反应编程中，我们只处理数据。换句话说，每次我们接收到新的数据，我们都认为这是一个事件。由于这个定义，您可以将它称为事件驱动编程的一个分支。然而，由于它的受欢迎程度和它所做事情的方式的不同，我不得不在自己的一章里把反应性的编程放在一个章节里。

在这一章中，我们将深入探讨Python中最流行的一个库，当它涉及到反应性编程时，RxPY。我们将深入介绍这个库的一些特性，以及如何利用它来创建我们自己的异步程序。

我们将从RxPY的一些基本需求开始着手:

处理观察和观察。
Lambda函数以及我们如何使用它们。
大量的运算符，以及如何将这些操作链锁起来以达到理想的状态。
冷热观察的区别。
多播
我们还将简要介绍PyFunctional库，以及它与RxPY的区别，以及我们如何在某些场景中利用它。你应该注意到，官方文档中的一些例子也被托马斯·尼德(Thomas Nield)在名为《数据科学》的视频课程中涵盖了。我强烈推荐这门课程，因为Thomas涵盖了很多我在这一章中没有机会学到的东西。你可以在http://shop.oreilly.com/product/0636920064237.do找到这门课程。